{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T \n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"ToxicTwitterComments\")\n",
    "spark = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Modify path to train.csv\n",
    "trainDF = spark.read.csv('test2/train.csv', \n",
    "                         header=True, \n",
    "                         multiLine=True, \n",
    "                         encoding=\"UTF-8\",\n",
    "                         sep=',',\n",
    "                         escape='\"',\n",
    "                         inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 95557\n",
      "Test Dataset Count: 64014\n"
     ]
    }
   ],
   "source": [
    "train, test = trainDF.randomSplit([0.6, 0.4], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.createOrReplaceTempView('train')\n",
    "test.createOrReplaceTempView('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- comment_text: string (nullable = true)\n",
      " |-- toxic: integer (nullable = true)\n",
      " |-- severe_toxic: integer (nullable = true)\n",
      " |-- obscene: integer (nullable = true)\n",
      " |-- threat: integer (nullable = true)\n",
      " |-- insult: integer (nullable = true)\n",
      " |-- identity_hate: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00024b59235015f3</td>\n",
       "      <td>Virgin\\nMy only warning? You'll block me? Well...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0002bfc2abe2a51f</td>\n",
       "      <td>\"*::::::::I believe that you're confusing \"\"pr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0002eeaf4c0cdf35</td>\n",
       "      <td>But isnt it against the rules to edit if you a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00030003d620f7a8</td>\n",
       "      <td>\"\\nseems about right. nableezy - \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "4  00024b59235015f3  Virgin\\nMy only warning? You'll block me? Well...      1   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  0002bfc2abe2a51f  \"*::::::::I believe that you're confusing \"\"pr...      0   \n",
       "8  0002eeaf4c0cdf35  But isnt it against the rules to edit if you a...      0   \n",
       "9  00030003d620f7a8                 \"\\nseems about right. nableezy - \"      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        1       0       1              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT * FROM train\n",
    "limit(10)\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(lines):\n",
    "    lines = lines.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    lines = lines.split(' ')\n",
    "    filtered_sentence = [w for w in lines if not w in stop_words]\n",
    "    cleaned_line = []\n",
    "    for w in filtered_sentence:\n",
    "        word = PorterStemmer().stem(w)\n",
    "        cleaned_line.append(word)\n",
    "    return ' '.join(cleaned_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(clean(\"'Hey man, I'm really not trying to edit war.'\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.clean(lines)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"clean\", clean, T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get rid of limit\n",
    "df_train=spark.sql(\"\"\"\n",
    "SELECT clean(comment_text) cleaned_comment, toxic, severe_toxic,obscene,threat,insult,identity_hate\n",
    "FROM train\n",
    "LIMIT 20000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning test data\n",
    "df_test=spark.sql(\"\"\"\n",
    "SELECT clean(comment_text) cleaned_comment, toxic, severe_toxic,obscene,threat,insult,identity_hate\n",
    "FROM test\n",
    "LIMIT 15000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get a column of array<string> for word2vec\n",
    "from pyspark.sql.functions import *\n",
    "df_train2=df_train.withColumn(\"cleaned_comment2\",split(df_train['cleaned_comment'], ' '))\n",
    "df_test2 = df_test.withColumn(\"cleaned_comment2\",split(df_test['cleaned_comment'], ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=30, seed=42, inputCol=\"cleaned_comment2\", outputCol=\"features\")\n",
    "#model = word2Vec.fit(df_train2)\n",
    "#model.getVectors().show()\n",
    "#wvfeature = model.transform(df_train2)\n",
    "\n",
    "\n",
    "pipeline_w2v = Pipeline(stages=[word2Vec])\n",
    "pipelineModel_w2v = pipeline_w2v.fit(df_train2)\n",
    "wvfeature = pipelineModel_w2v.transform(df_train2)\n",
    "wvfeature_test = pipelineModel_w2v.transform(df_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wvfeature.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the Word2Vec featuresï¼Œ we can follow the procedure like TF-IDF to build another model and compare the performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.getrecursionlimit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.setrecursionlimit(30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data processing pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_comment\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
    "pipelineModel = pipeline.fit(df_train)\n",
    "rescaledData = pipelineModel.transform(df_train)\n",
    "rescaledData_test = pipelineModel.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save as Parquet for faster read\n",
    "#rescaledData.write.parquet(\"rescaledData.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rescaledData = spark.read.parquet(\"rescaledData.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelling (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Training, Testing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust weight to balance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get weight to adjust unbalanced\n",
    "def weight_ratio(predictions,class_name):\n",
    "    pos_lab=predictions[predictions[class_name]==1].count()\n",
    "    neg_lab=predictions[predictions[class_name]==0].count()\n",
    "    return neg_lab/pos_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "def recall_precision(predictions):\n",
    "    TP=predictions[(predictions.label==1)&(predictions.prediction==1)].count()\n",
    "    FP=predictions[(predictions.label==0)&(predictions.prediction==1)].count()\n",
    "    TN=predictions[(predictions.label==0)&(predictions.prediction==0)].count()\n",
    "    FN=predictions[(predictions.label==1)&(predictions.prediction==0)].count()\n",
    "    recall=TP/(TP+FN)\n",
    "    precision=TP/(TP+FP)\n",
    "    f1=2 * precision * recall / (precision + recall)\n",
    "    return recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = [\"toxic\", 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification model for toxic\n",
      "Test Area Under ROC: 0.8285462173184769\n",
      "Precision: 0.4675925925925926\n",
      "Recall: 0.5422818791946309\n",
      "F1 score: 0.5021752641392169\n",
      "Classification model for severe_toxic\n",
      "Test Area Under ROC: 0.8220228136342548\n",
      "Precision: 0.12786885245901639\n",
      "Recall: 0.5131578947368421\n",
      "F1 score: 0.2047244094488189\n",
      "Classification model for obscene\n",
      "Test Area Under ROC: 0.8169618887586468\n",
      "Precision: 0.2989771833202203\n",
      "Recall: 0.4791929382093317\n",
      "F1 score: 0.3682170542635658\n",
      "Classification model for threat\n",
      "Test Area Under ROC: 0.7855831651632627\n",
      "Precision: 0.11764705882352941\n",
      "Recall: 0.18181818181818182\n",
      "F1 score: 0.14285714285714285\n",
      "Classification model for insult\n",
      "Test Area Under ROC: 0.8316064261375564\n",
      "Precision: 0.3494874184529357\n",
      "Recall: 0.49407114624505927\n",
      "F1 score: 0.40938864628820953\n",
      "Classification model for identity_hate\n",
      "Test Area Under ROC: 0.8021161983026731\n",
      "Precision: 0.20100502512562815\n",
      "Recall: 0.30303030303030304\n",
      "F1 score: 0.24169184290030213\n"
     ]
    }
   ],
   "source": [
    "for name in class_name:\n",
    "    train=rescaledData.select(\"features\",name)\n",
    "    test = rescaledData_test.select(\"features\",name)\n",
    "    ratio=weight_ratio(train,name)\n",
    "    train=train.withColumn(\"weight\", F.when(train[name]==1,ratio).otherwise(1))\n",
    "    ##changed maxIter to 5 for faster calculation\n",
    "    lr = LogisticRegression(featuresCol = 'features',weightCol=\"weight\",labelCol = name, maxIter=10)\n",
    "    lrModel = lr.fit(train)\n",
    "    #save model, not yet tested\n",
    "    #lrModel.save([spark_context], [file_path])\n",
    "    predictions = lrModel.transform(test)\n",
    "    #predictions.select(class_name, 'rawPrediction', 'prediction', 'probability').show(10)\n",
    "    predictions=predictions.select(predictions[name].alias(\"label\"), 'rawPrediction', 'prediction', 'probability')\n",
    "    print('Classification model for ' + str(name))\n",
    "    print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))\n",
    "    recall, precision, f1 = recall_precision(predictions)\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"F1 score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification model for toxic\n",
      "Test Area Under ROC: 0.9232691170845194\n",
      "Precision: 0.4505791505791506\n",
      "Recall: 0.7832214765100671\n",
      "F1 score: 0.5720588235294117\n",
      "Classification model for severe_toxic\n",
      "Test Area Under ROC: 0.9746527974704784\n",
      "Precision: 0.1499460625674218\n",
      "Recall: 0.9144736842105263\n",
      "F1 score: 0.25764596848934196\n",
      "Classification model for obscene\n",
      "Test Area Under ROC: 0.9540816557492106\n",
      "Precision: 0.38648180242634317\n",
      "Recall: 0.8436317780580076\n",
      "F1 score: 0.5301109350237718\n",
      "Classification model for threat\n",
      "Test Area Under ROC: 0.9215167521699208\n",
      "Precision: 0.02328508495909377\n",
      "Recall: 0.8409090909090909\n",
      "F1 score: 0.0453153704837722\n",
      "Classification model for insult\n",
      "Test Area Under ROC: 0.9420444819690497\n",
      "Precision: 0.33496999454446263\n",
      "Recall: 0.8089591567852438\n",
      "F1 score: 0.4737654320987654\n",
      "Classification model for identity_hate\n",
      "Test Area Under ROC: 0.9339312210076061\n",
      "Precision: 0.06905370843989769\n",
      "Recall: 0.8181818181818182\n",
      "F1 score: 0.12735849056603774\n"
     ]
    }
   ],
   "source": [
    "for name in class_name:\n",
    "    train=wvfeature.select(\"features\",name)\n",
    "    test = wvfeature_test.select(\"features\",name)\n",
    "    ratio=weight_ratio(train,name)\n",
    "    train=train.withColumn(\"weight\", F.when(train[name]==1,ratio).otherwise(1))\n",
    "    ##changed maxIter to 5 for faster calculation\n",
    "    lr = LogisticRegression(featuresCol = 'features',weightCol=\"weight\",labelCol = name,maxIter=10)\n",
    "    lrModel = lr.fit(train)\n",
    "    #save model, not yet tested\n",
    "    #lrModel.save([spark_context], [file_path])\n",
    "    predictions = lrModel.transform(test)\n",
    "    #predictions.select(class_name, 'rawPrediction', 'prediction', 'probability').show(10)\n",
    "    predictions=predictions.select(predictions[name].alias(\"label\"), 'rawPrediction', 'prediction', 'probability')\n",
    "    print('Classification model for ' + str(name))\n",
    "    print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))\n",
    "    recall, precision, f1 = recall_precision(predictions)\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"F1 score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Pyspark",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
