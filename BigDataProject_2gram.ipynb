{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import *\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T \n",
    "import string\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "sc = pyspark.SparkContext(appName=\"ToxicTwitterComments\")\n",
    "spark = pyspark.sql.SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF = spark.read.csv('train.csv', \n",
    "                         header=True, \n",
    "                         multiLine=True, \n",
    "                         encoding=\"UTF-8\",\n",
    "                         sep=',',\n",
    "                         escape='\"',\n",
    "                         inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- comment_text: string (nullable = true)\n",
      " |-- toxic: integer (nullable = true)\n",
      " |-- severe_toxic: integer (nullable = true)\n",
      " |-- obscene: integer (nullable = true)\n",
      " |-- threat: integer (nullable = true)\n",
      " |-- insult: integer (nullable = true)\n",
      " |-- identity_hate: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 95557\n",
      "Test Dataset Count: 64014\n"
     ]
    }
   ],
   "source": [
    "train, test = trainDF.randomSplit([0.6, 0.4], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.createOrReplaceTempView('train')\n",
    "test.createOrReplaceTempView('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00024b59235015f3</td>\n",
       "      <td>Virgin\\nMy only warning? You'll block me? Well...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00025465d4725e87</td>\n",
       "      <td>\"\\n\\nCongratulations from me as well, use the ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0002bcb3da6cb337</td>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0002bfc2abe2a51f</td>\n",
       "      <td>\"*::::::::I believe that you're confusing \"\"pr...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0002eeaf4c0cdf35</td>\n",
       "      <td>But isnt it against the rules to edit if you a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00030003d620f7a8</td>\n",
       "      <td>\"\\nseems about right. nableezy - \"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "4  00024b59235015f3  Virgin\\nMy only warning? You'll block me? Well...      1   \n",
       "5  00025465d4725e87  \"\\n\\nCongratulations from me as well, use the ...      0   \n",
       "6  0002bcb3da6cb337       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1   \n",
       "7  0002bfc2abe2a51f  \"*::::::::I believe that you're confusing \"\"pr...      0   \n",
       "8  0002eeaf4c0cdf35  But isnt it against the rules to edit if you a...      0   \n",
       "9  00030003d620f7a8                 \"\\nseems about right. nableezy - \"      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        1       0       1              0  \n",
       "5             0        0       0       0              0  \n",
       "6             1        1       0       1              0  \n",
       "7             0        0       0       0              0  \n",
       "8             0        0       0       0              0  \n",
       "9             0        0       0       0              0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('''\n",
    "SELECT * FROM train\n",
    "limit(10)\n",
    "''').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(lines):\n",
    "    lines = lines.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    stop_words = list(set(stopwords.words('english')))\n",
    "    stop_words.remove('not')\n",
    "    word_tokens = word_tokenize(lines)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    cleaned_line = []\n",
    "    for w in filtered_sentence:\n",
    "        word = PorterStemmer().stem(w)\n",
    "        cleaned_line.append(word)\n",
    "    return ' '.join(cleaned_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hey man im realli not tri edit war'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean(\"'Hey man, I'm really not trying to edit war.'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.clean(lines)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.udf.register(\"clean\", clean, T.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning test data\n",
    "df_test=spark.sql(\"\"\"\n",
    "SELECT clean(comment_text) cleaned_comment, toxic, severe_toxic,obscene,threat,insult,identity_hate\n",
    "FROM test\n",
    "LIMIT 10000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=spark.sql(\"\"\"\n",
    "SELECT clean(comment_text) cleaned_comment, toxic, severe_toxic,obscene,threat,insult,identity_hate\n",
    "FROM train\n",
    "limit 10000\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.mllib.feature import HashingTF, IDF\n",
    "from __future__ import print_function\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "#data processing pipeline\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_comment\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf])\n",
    "pipelineModel = pipeline.fit(df_train)\n",
    "rescaledData = pipelineModel.transform(df_train)\n",
    "rescaledData_test = pipelineModel.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=2, inputCol=tokenizer.getOutputCol(), outputCol=\"ngrams\")\n",
    "hashingTF_2gram = HashingTF(inputCol=ngram.getOutputCol(), outputCol=\"rawFeatures_2gram\")\n",
    "idf_2gram = IDF(inputCol=\"rawFeatures_2gram\", outputCol=\"features_2gram\")\n",
    "\n",
    "pipeline_2gram = Pipeline(stages=[tokenizer, ngram, hashingTF_2gram, idf_2gram])\n",
    "pipelineModel_2gram = pipeline_2gram.fit(df_train)\n",
    "rescaledData_2gram = pipelineModel_2gram.transform(df_train)\n",
    "rescaledData_test_2gram = pipelineModel_2gram.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get weight to adjust unbalanced\n",
    "def weight_ratio(predictions,class_name):\n",
    "    pos_lab=predictions[predictions[class_name]==1].count()\n",
    "    neg_lab=predictions[predictions[class_name]==0].count()\n",
    "    return neg_lab/pos_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData.createOrReplaceTempView('table1')\n",
    "rescaledData_2gram.createOrReplaceTempView('table2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescaledData_test.createOrReplaceTempView('table3')\n",
    "rescaledData_test_2gram.createOrReplaceTempView('table4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_all = spark.sql('select table1.toxic, table1.obscene, table1.threat, table1.insult, table1.identity_hate,  table1.severe_toxic, table1.features, features_2gram from table1 left join table2 on (table1.cleaned_comment = table2.cleaned_comment)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_all_test = spark.sql('select table3.toxic, table3.obscene, table3.threat, table3.insult, table3.identity_hate,  table3.severe_toxic, table3.features, features_2gram from table3 left join table4 on (table3.cleaned_comment = table4.cleaned_comment)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"features_2gram\", \"features\"],\n",
    "    outputCol=\"features_all\")\n",
    "rescaledData_all = assembler.transform(vector_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rescaledData_all_test = assembler.transform(vector_all_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modelling (LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "def recall_precision(predictions):\n",
    "    TP=predictions[(predictions.label==1)&(predictions.prediction==1)].count()\n",
    "    FP=predictions[(predictions.label==0)&(predictions.prediction==1)].count()\n",
    "    TN=predictions[(predictions.label==0)&(predictions.prediction==0)].count()\n",
    "    FN=predictions[(predictions.label==1)&(predictions.prediction==0)].count()\n",
    "    recall=TP/(TP+FN)\n",
    "    precision=TP/(TP+FP)\n",
    "    f1=2 * precision * recall / (precision + recall)\n",
    "    return recall, precision, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_name = [\"toxic\", 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification model for toxic\n",
      "Test Area Under ROC: 0.848597111759121\n",
      "Precision: 0.20742213386348576\n",
      "Recall: 0.8402684563758389\n",
      "F1 score: 0.332713260696253\n",
      "Classification model for severe_toxic\n",
      "Test Area Under ROC: 0.942338060770156\n",
      "Precision: 0.20607375271149675\n",
      "Recall: 0.625\n",
      "F1 score: 0.3099510603588907\n",
      "Classification model for obscene\n",
      "Test Area Under ROC: 0.8910953261677094\n",
      "Precision: 0.5727513227513228\n",
      "Recall: 0.5460277427490542\n",
      "F1 score: 0.5590703679793415\n",
      "Classification model for threat\n",
      "Test Area Under ROC: 0.8319449172116989\n",
      "Precision: 0.017482517482517484\n",
      "Recall: 0.11363636363636363\n",
      "F1 score: 0.030303030303030304\n",
      "Classification model for insult\n",
      "Test Area Under ROC: 0.8891314200800602\n",
      "Precision: 0.4133083411433927\n",
      "Recall: 0.5810276679841897\n",
      "F1 score: 0.4830230010952903\n",
      "Classification model for identity_hate\n",
      "Test Area Under ROC: 0.9166439923855264\n",
      "Precision: 0.10935441370223979\n",
      "Recall: 0.6287878787878788\n",
      "F1 score: 0.186307519640853\n"
     ]
    }
   ],
   "source": [
    "for name in class_name:\n",
    "    train=rescaledData.select(\"features\",name)\n",
    "    test = rescaledData_test.select(\"features\",name)\n",
    "    ratio=weight_ratio(train,name)\n",
    "    train=train.withColumn(\"weight\", F.when(train[name]==1,ratio).otherwise(1))\n",
    "    ##changed maxIter to 5 for faster calculation\n",
    "    lr = LogisticRegression(featuresCol = 'features',weightCol=\"weight\",labelCol = name, maxIter=10,  regParam=0.03, elasticNetParam=1)\n",
    "    lrModel = lr.fit(train)\n",
    "    #save model, not yet tested\n",
    "    #lrModel.save([spark_context], [file_path])\n",
    "    predictions = lrModel.transform(test)\n",
    "    #predictions.select(class_name, 'rawPrediction', 'prediction', 'probability').show(10)\n",
    "    predictions=predictions.select(predictions[name].alias(\"label\"), 'rawPrediction', 'prediction', 'probability')\n",
    "    print('Classification model for ' + str(name))\n",
    "    print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))\n",
    "    recall, precision, f1 = recall_precision(predictions)\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"F1 score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification model for toxic\n",
      "Test Area Under ROC: 0.5593419242023056\n",
      "Precision: 0.10686188811188811\n",
      "Recall: 0.9845637583892617\n",
      "F1 score: 0.19279800236561967\n",
      "Classification model for severe_toxic\n",
      "Test Area Under ROC: 0.730005946219938\n",
      "Precision: 0.2109375\n",
      "Recall: 0.17763157894736842\n",
      "F1 score: 0.19285714285714284\n",
      "Classification model for obscene\n",
      "Test Area Under ROC: 0.6132865607783878\n",
      "Precision: 0.5987261146496815\n",
      "Recall: 0.11853720050441362\n",
      "F1 score: 0.19789473684210523\n",
      "Classification model for threat\n",
      "Test Area Under ROC: 0.6503303630042103\n",
      "Precision: 0.15384615384615385\n",
      "Recall: 0.045454545454545456\n",
      "F1 score: 0.07017543859649122\n",
      "Classification model for insult\n",
      "Test Area Under ROC: 0.5971463473821953\n",
      "Precision: 0.05807275047862157\n",
      "Recall: 0.9591567852437418\n",
      "F1 score: 0.1095148552087251\n",
      "Classification model for identity_hate\n",
      "Test Area Under ROC: 0.5756487901615018\n",
      "Precision: 0.0851063829787234\n",
      "Recall: 0.09090909090909091\n",
      "F1 score: 0.08791208791208792\n"
     ]
    }
   ],
   "source": [
    "for name in class_name:\n",
    "    train=rescaledData_2gram.select(\"features_2gram\",name)\n",
    "    test = rescaledData_test_2gram.select(\"features_2gram\",name)\n",
    "    ratio=weight_ratio(train,name)\n",
    "    train=train.withColumn(\"weight\", F.when(train[name]==1,ratio).otherwise(1))\n",
    "    ##changed maxIter to 5 for faster calculation\n",
    "    lr = LogisticRegression(featuresCol = 'features_2gram',weightCol=\"weight\",labelCol = name, maxIter=10,regParam=0.03, elasticNetParam=1)\n",
    "    lrModel = lr.fit(train)\n",
    "    #save model, not yet tested\n",
    "    #lrModel.save([spark_context], [file_path])\n",
    "    predictions = lrModel.transform(test)\n",
    "    #predictions.select(class_name, 'rawPrediction', 'prediction', 'probability').show(10)\n",
    "    predictions=predictions.select(predictions[name].alias(\"label\"), 'rawPrediction', 'prediction', 'probability')\n",
    "    print('Classification model for ' + str(name))\n",
    "    print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))\n",
    "    recall, precision, f1 = recall_precision(predictions)\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"F1 score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification model for toxic\n",
      "Test Area Under ROC: 0.8538930843934898\n",
      "Precision: 0.21015820149875103\n",
      "Recall: 0.8447121820615796\n",
      "F1 score: 0.3365782104280571\n",
      "Classification model for severe_toxic\n",
      "Test Area Under ROC: 0.9465591972711515\n",
      "Precision: 0.1800356506238859\n",
      "Recall: 0.6644736842105263\n",
      "F1 score: 0.28330995792426367\n",
      "Classification model for obscene\n",
      "Test Area Under ROC: 0.892818984011728\n",
      "Precision: 0.5241635687732342\n",
      "Recall: 0.7085427135678392\n",
      "F1 score: 0.6025641025641025\n",
      "Classification model for threat\n",
      "Test Area Under ROC: 0.8463264798942696\n",
      "Precision: 0.09859154929577464\n",
      "Recall: 0.1590909090909091\n",
      "F1 score: 0.1217391304347826\n",
      "Classification model for insult\n",
      "Test Area Under ROC: 0.890067033553443\n",
      "Precision: 0.1368909512761021\n",
      "Recall: 0.8528252299605782\n",
      "F1 score: 0.2359142130134497\n",
      "Classification model for identity_hate\n",
      "Test Area Under ROC: 0.9117853586163541\n",
      "Precision: 0.10096153846153846\n",
      "Recall: 0.6363636363636364\n",
      "F1 score: 0.17427385892116182\n"
     ]
    }
   ],
   "source": [
    "for name in class_name:\n",
    "    train=rescaledData_all.select(\"features_all\",name)\n",
    "    test = rescaledData_all_test.select(\"features_all\",name)\n",
    "    ratio=weight_ratio(train,name)\n",
    "    train=train.withColumn(\"weight\", F.when(train[name]==1,ratio).otherwise(1))\n",
    "    ##changed maxIter to 5 for faster calculation\n",
    "    lr = LogisticRegression(featuresCol = 'features_all',weightCol=\"weight\",labelCol = name, maxIter=10,regParam=0.03, elasticNetParam=1)\n",
    "    lrModel = lr.fit(train)\n",
    "    #save model, not yet tested\n",
    "    #lrModel.save([spark_context], [file_path])\n",
    "    predictions = lrModel.transform(test)\n",
    "    #predictions.select(class_name, 'rawPrediction', 'prediction', 'probability').show(10)\n",
    "    predictions=predictions.select(predictions[name].alias(\"label\"), 'rawPrediction', 'prediction', 'probability')\n",
    "    print('Classification model for ' + str(name))\n",
    "    print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))\n",
    "    recall, precision, f1 = recall_precision(predictions)\n",
    "    print(\"Precision: \" + str(precision))\n",
    "    print(\"Recall: \" + str(recall))\n",
    "    print(\"F1 score: \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load documents (one per line).\n",
    "#documents = sc.textFile(\"data/mllib/kmeans_data.txt\").map(lambda line: line.split(\" \"))\n",
    "\n",
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(df_train.rdd)\n",
    "\n",
    "# While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n",
    "# First to compute the IDF vector and second to scale the term frequencies by IDF.\n",
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "\n",
    "# spark.mllib's IDF implementation provides an option for ignoring terms\n",
    "# which occur in less than a minimum number of documents.\n",
    "# In such cases, the IDF for these terms is set to 0.\n",
    "# This feature can be used by passing the minDocFreq value to the IDF constructor.\n",
    "idfIgnore = IDF(minDocFreq=2).fit(tf)\n",
    "tfidfIgnore = idfIgnore.transform(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
